Como mencioné anteriormente, los buscadores cuentan con robots que, de manera automatizada, recorren las páginas web, revisan su contenido y lo almacenan en sus propias bases de datos.

En muchas ocasiones, los buscadores aplican la regla de que si les ayudas en su trabajo, ellos te ayudarán a ti indexándote de mejor manera.

Por lo tanto, en ocasiones un robot que visita nuestra web no cuenta con suficiente información para saber exactamente qué páginas queremos que sean indexadas y cuáles no, y cómo queremos que se reindexe nuestra página web.

Especialmente en el caso de Google, se recomienda incluir un archivo de texto llamado "robots.txt" en la raíz de la página web en la que estamos trabajando.

Este archivo debe contener información básica para especificar si la página debe ser indexada o no, y en qué condiciones.

Es importante destacar que este archivo no obliga al buscador a indexar la página de una forma u otra, pero definitivamente ayuda a facilitar su tarea.